<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>In-depth Study - ELK, Kafka, Spark · J. An's Blog</title><meta name="description" content="The in-depth study documentation for the graduate level course COMS 6998: Cloud Computing &amp;amp; Big Data taken in Fall 2020 @ Columbia. Topics include"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/blog/css/bootstrap.min.css"><link rel="stylesheet" href="/blog/css/font-awesome.min.css"><link rel="stylesheet" href="/blog/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        }
    })

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><meta name="generator" content="Hexo 5.2.0"><link rel="stylesheet" href="/blog/css/prism-base16-ateliersulphurpool.light.css" type="text/css">
<link rel="stylesheet" href="/blog/css/prism-line-numbers.css" type="text/css"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/blog/" class="a-title"></a></h3><h1 tabindex="-1" class="site-title-large"><a href="/blog/" class="a-title">J. An's Blog</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/blog/">Home</a></li><li><a href="/blog/archives">Archive</a></li><li><a href="/blog/tags">Tags</a></li><li class="soc"><a href="https://github.com/j-an-dev" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a><a href="https://twitter.com/anjie_tweet" target="_blank" rel="noopener noreferrer"><i class="fa fa-twitter">&nbsp;</i></a><a href="https://www.instagram.com/aj_ins" target="_blank" rel="noopener noreferrer"><i class="fa fa-instagram">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2021&nbsp;<a target="_blank" href="https://j-an.org/" rel="noopener noreferrer">J. An</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>In-depth Study - ELK, Kafka, Spark</a></p><p class="post-meta"><span class="date meta-item">Posted on&nbsp;2020-12-01</span></p><p class="post-abstract"><p>The in-depth study documentation for the graduate level course COMS 6998: Cloud Computing &amp; Big Data taken in Fall 2020 @ Columbia. Topics include the ELK Stack, Kafka (w/ AWS MSK), Spark (w/ AWS EMR) and an end-to-end Real-time Credit Card Fraud Detection Pipeline implementation.<a id="more"></a></p>
</br>

<h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents "></a>Table of Contents <!-- omit in toc --></h2><ul>
<li><a href="#1-task-1-elk-stack">1. Task 1: ELK Stack</a><ul>
<li><a href="#11-elk-stack-installation">1.1. ELK Stack Installation</a></li>
<li><a href="#12-example-of-a-logstash-pipeline-sending-syslog-logs-into-the-stack">1.2. Example of a Logstash pipeline sending syslog logs into the Stack</a><ul>
<li><a href="#121-implementation-details">1.2.1. Implementation details</a><ul>
<li><a href="#1211-start-services-elasticsearch-and-kibana">1.2.1.1. Start services Elasticsearch and Kibana</a></li>
<li><a href="#1212-create-a-new-logstash-configuration-file">1.2.1.2. Create a new Logstash configuration file</a></li>
<li><a href="#1213-start-the-logstash-pipeline">1.2.1.3. Start the Logstash pipeline</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-task-2-kafka-setup">2. Task 2: Kafka Setup</a><ul>
<li><a href="#21-kafka-installation">2.1. Kafka Installation</a></li>
<li><a href="#22-example-of-a-kafka-messages-producing-and-consuming-pipeline">2.2. Example of a Kafka messages producing and consuming pipeline</a><ul>
<li><a href="#221-create-a-kafka-topic-to-store-events">2.2.1. Create a Kafka topic to store events</a></li>
<li><a href="#222-produce-messages-to-the-created-topic">2.2.2. Produce messages to the created topic</a></li>
<li><a href="#223-consume-the-messages-produced-by-the-producer">2.2.3. Consume the messages produced by the producer</a></li>
</ul>
</li>
<li><a href="#23-create-a-kafka-pipeline-on-aws-msk">2.3. Create a Kafka pipeline on AWS MSK</a><ul>
<li><a href="#231-vpc-network-configuration">2.3.1. VPC network configuration</a></li>
<li><a href="#232-create-a-kafka-cluster-on-aws-msk-service">2.3.2. Create a Kafka cluster on AWS MSK service</a></li>
<li><a href="#233-create-an-ec2-instance-and-connect-to-the-kafka-cluster">2.3.3. Create an EC2 instance and connect to the Kafka cluster</a></li>
<li><a href="#234-produce-and-consume-messages-from-the-topic">2.3.4. Produce and consume messages from the topic</a><ul>
<li><a href="#2341-produce-messages-to-the-created-topic">2.3.4.1. Produce messages to the created topic</a></li>
<li><a href="#2342-consume-the-messages-produced-by-the-producer">2.3.4.2. Consume the messages produced by the producer</a></li>
</ul>
</li>
<li><a href="#235-using-amazon-msk-as-an-event-source-for-aws-lambda">2.3.5. Using Amazon MSK as an event source for AWS Lambda</a><ul>
<li><a href="#2351-required-lambda-function-permissions">2.3.5.1. Required Lambda function permissions</a></li>
<li><a href="#2352-configure-the-lambda-event-source-mapping">2.3.5.2. Configure the Lambda event source mapping</a></li>
<li><a href="#2353-produce-a-message-in-the-topic-to-trigger-the-lambda-function">2.3.5.3. Produce a message in the topic to trigger the Lambda function</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#3-task-3-spark-setup">3. Task 3: Spark Setup</a><ul>
<li><a href="#31-spark-installation">3.1. Spark Installation</a></li>
<li><a href="#32-working-with-aws-emr">3.2. Working with AWS EMR</a><ul>
<li><a href="#321-create-a-spark-cluster-on-aws-emr">3.2.1. Create a Spark cluster on AWS EMR</a></li>
<li><a href="#322-working-with-emr-notebooks">3.2.2. Working with EMR Notebooks</a></li>
<li><a href="#323-submit-a-pyspark-job-to-the-emr-cluster">3.2.3. Submit a PySpark job to the EMR cluster</a><ul>
<li><a href="#3231-create-the-job-script-and-save-it-in-the-s3-bucket">3.2.3.1. Create the job script and save it in the S3 bucket</a></li>
<li><a href="#3232-create-a-input-text-and-save-it-in-the-s3-bucket">3.2.3.2. Create a input text and save it in the S3 bucket</a></li>
<li><a href="#3233-submit-the-job-as-a-step-to-the-cluster">3.2.3.3. Submit the job as a step to the cluster</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-task-4-real-time-credit-card-fraud-detection-pipeline">4. Task 4: Real-time Credit Card Fraud Detection Pipeline</a><ul>
<li><a href="#41-workflow-and-architecture">4.1. Workflow and Architecture</a></li>
<li><a href="#42-implementation-details">4.2. Implementation Details</a><ul>
<li><a href="#421-customers--transactions-dataset">4.2.1. Customers &amp; Transactions dataset</a></li>
<li><a href="#422-spark-ml-job">4.2.2. Spark ML job</a></li>
<li><a href="#423-kafka-producer">4.2.3. Kafka producer</a></li>
<li><a href="#424-spark-streaming-job">4.2.4. Spark Streaming job</a></li>
<li><a href="#425-front-end-dashboard">4.2.5. Front-end dashboard</a></li>
<li><a href="#426-rest-api-for-customers-and-transaction-statements">4.2.6. REST API for customers and transaction statements</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</br>

<h1 id="1-Task-1-ELK-Stack"><a href="#1-Task-1-ELK-Stack" class="headerlink" title="1. Task 1: ELK Stack"></a>1. Task 1: ELK Stack</h1><h2 id="1-1-ELK-Stack-Installation"><a href="#1-1-ELK-Stack-Installation" class="headerlink" title="1.1. ELK Stack Installation"></a>1.1. ELK Stack Installation</h2><p>I followed the tutorial <a target="_blank" rel="noopener" href="https://logz.io/learn/complete-guide-elk-stack/">The Complete Guide to the ELK Stack</a> to digest the basic concepts of the ELK Stack and tutorial <a target="_blank" rel="noopener" href="https://logz.io/blog/elk-mac/">Installing the ELK Stack on Mac OS X</a> to install the required services using Homebrew. The screenshot below reflects the success of installation.</p>
<p><img src="/blog/indepth-study/installation.png" alt="installation"></p>
<h2 id="1-2-Example-of-a-Logstash-pipeline-sending-syslog-logs-into-the-Stack"><a href="#1-2-Example-of-a-Logstash-pipeline-sending-syslog-logs-into-the-Stack" class="headerlink" title="1.2. Example of a Logstash pipeline sending syslog logs into the Stack"></a>1.2. Example of a Logstash pipeline sending syslog logs into the Stack</h2><p>In this example, I implemented a Logstash pipeline that collects system log data and shipping them into the Stack. Then, created the index pattern <code>syslog-demo</code> in Kibana to visualize the syslog data.</p>
<h3 id="1-2-1-Implementation-details"><a href="#1-2-1-Implementation-details" class="headerlink" title="1.2.1. Implementation details"></a>1.2.1. Implementation details</h3><h4 id="1-2-1-1-Start-services-Elasticsearch-and-Kibana"><a href="#1-2-1-1-Start-services-Elasticsearch-and-Kibana" class="headerlink" title="1.2.1.1. Start services Elasticsearch and Kibana"></a>1.2.1.1. Start services Elasticsearch and Kibana</h4><pre><code>brew services start elasticsearch
brew services start kibana</code></pre>
<h4 id="1-2-1-2-Create-a-new-Logstash-configuration-file"><a href="#1-2-1-2-Create-a-new-Logstash-configuration-file" class="headerlink" title="1.2.1.2. Create a new Logstash configuration file"></a>1.2.1.2. Create a new Logstash configuration file</h4><pre><code># Make the directory
sudo mkdir -p /etc/logstash/conf.d
# Locate to the configuration folder
cd /etc/logstash/conf.d
# Create the conf file
sudo vim  /etc/logstash/conf.d/syslog.conf</code></pre>
<p>Enter the following configuration in the <code>syslog.conf</code> file:</p>
<pre><code>```
input &#123;
    file &#123;
        path =&gt; [ &quot;/var/log/*.log&quot;, &quot;/var/log/messages&quot;, &quot;/var/log/syslog&quot; ]
        type =&gt; &quot;syslog&quot;
    &#125;
&#125;

filter &#123;
    if [type] == &quot;syslog&quot; &#123;
        grok &#123;
        match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGTIMESTAMP:syslog_timestamp&#125; %&#123;SYSLOGHOST:syslog_hostname&#125; %&#123;DATA:syslog_program&#125;(?:\[%&#123;POSINT:syslog_pid&#125;\])?: %&#123;GREEDYDATA:syslog_message&#125;&quot; &#125;
        add_field =&gt; [ &quot;received_at&quot;, &quot;%&#123;@timestamp&#125;&quot; ]
        add_field =&gt; [ &quot;received_from&quot;, &quot;%&#123;host&#125;&quot; ]
        &#125;
        syslog_pri &#123; &#125;
        date &#123;
        match =&gt; [ &quot;syslog_timestamp&quot;, &quot;MMM  d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ]
        &#125;
    &#125;
&#125;

output &#123;
    elasticsearch &#123;
        hosts =&gt; [&quot;127.0.0.1:9200&quot;] 
        index =&gt; &quot;syslog-demo&quot;
    &#125;
    stdout &#123; codec =&gt; rubydebug &#125;
&#125;
```</code></pre>
<h4 id="1-2-1-3-Start-the-Logstash-pipeline"><a href="#1-2-1-3-Start-the-Logstash-pipeline" class="headerlink" title="1.2.1.3. Start the Logstash pipeline"></a>1.2.1.3. Start the Logstash pipeline</h4><p>Make sure under the directory <code>/etc/logstash/conf.d/</code>, then run <code>logstash -f syslog.conf</code> to start the pipeline giving the following logs:<br><img src="/blog/indepth-study/logstash_pipeline.png" alt="Logstach Pipeline"></p>
<p>In Kibana (by accessing <a target="_blank" rel="noopener" href="https://localhost:5601/">https://localhost:5601</a>), initialized an index pattern with the index name <code>syslog-demo</code> created by the Logstash pipeline and selected <code>@timestamp</code> field as the Time Filter field name.<br><img src="/blog/indepth-study/kibana_index_pattern.png" alt="Kibana Index Pattern"></p>
<p>Under the <code>Discover</code> page, syslog data could be found:<br><img src="/blog/indepth-study/discover_log.png" alt="Discover Logs"></p>
<h1 id="2-Task-2-Kafka-Setup"><a href="#2-Task-2-Kafka-Setup" class="headerlink" title="2. Task 2: Kafka Setup"></a>2. Task 2: Kafka Setup</h1><h2 id="2-1-Kafka-Installation"><a href="#2-1-Kafka-Installation" class="headerlink" title="2.1. Kafka Installation"></a>2.1. Kafka Installation</h2><p>Working on Mac, I choose to use Homebrew for installation:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># This will install Java 1.8, Kafka, ZooKeeper at the same time</span>
brew install kafka

<span class="token comment" spellcheck="true"># This will run ZooKeeper and Kafka as services</span>
brew services start zookeeper
brew services start kafka<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>The screenshot below reflects the success of installation.<br><img src="/blog/indepth-study/kafka_installation.png" alt="kafka_installation"></p>
<h2 id="2-2-Example-of-a-Kafka-messages-producing-and-consuming-pipeline"><a href="#2-2-Example-of-a-Kafka-messages-producing-and-consuming-pipeline" class="headerlink" title="2.2. Example of a Kafka messages producing and consuming pipeline"></a>2.2. Example of a Kafka messages producing and consuming pipeline</h2><h3 id="2-2-1-Create-a-Kafka-topic-to-store-events"><a href="#2-2-1-Create-a-Kafka-topic-to-store-events" class="headerlink" title="2.2.1. Create a Kafka topic to store events"></a>2.2.1. Create a Kafka topic to store events</h3><p>I name the topic as <code>quickstart</code>. And following code initialize that topic with 1 partition and 1 replication factor:</p>
<pre><code>kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic quickstart</code></pre>
<h3 id="2-2-2-Produce-messages-to-the-created-topic"><a href="#2-2-2-Produce-messages-to-the-created-topic" class="headerlink" title="2.2.2. Produce messages to the created topic"></a>2.2.2. Produce messages to the created topic</h3><p>Initialize the Kafka producer console, which will listen to localhost at port 9092 at topic <code>quickstart</code>. And then input three messages:</p>
<pre><code>kafka-console-producer --bootstrap-server localhost:9092 --topic quickstart
&gt;This is the first event.
&gt;This is the second event.
&gt;This is the third event.</code></pre>
<h3 id="2-2-3-Consume-the-messages-produced-by-the-producer"><a href="#2-2-3-Consume-the-messages-produced-by-the-producer" class="headerlink" title="2.2.3. Consume the messages produced by the producer"></a>2.2.3. Consume the messages produced by the producer</h3><p>Open another Terminal window and initialize the Kafka consumer console, which will listen to bootstrap server localhost at port 9092 at topic <code>quickstart</code> from beginning:</p>
<pre><code>kafka-console-consumer --bootstrap-server localhost:9092 --topic quickstart --from-beginning</code></pre>
<p>The screenshot below reflects that the consumer successfully received the messages:<br><img src="/blog/indepth-study/events.png" alt="events"></p>
<h2 id="2-3-Create-a-Kafka-pipeline-on-AWS-MSK"><a href="#2-3-Create-a-Kafka-pipeline-on-AWS-MSK" class="headerlink" title="2.3. Create a Kafka pipeline on AWS MSK"></a>2.3. Create a Kafka pipeline on AWS MSK</h2><h3 id="2-3-1-VPC-network-configuration"><a href="#2-3-1-VPC-network-configuration" class="headerlink" title="2.3.1. VPC network configuration"></a>2.3.1. VPC network configuration</h3><p>Amazon MSK is a highly available service, so it must be configured to run in a minimum of two Availability Zones in the preferred Region. To comply with security best practice, the brokers are usually configured in private subnets in each Region.</p>
<p>For later one Amazon MSK to invoke Lambda, must ensure that there is a NAT Gateway running in the public subnet of each Region. I configure a new VPC with public and private subnets in two Availability Zones using <a href="/blog/indepth-study/vpc-msk.txt">this AWS CloudFormation template</a>. In this configuration, the public subnets are set up to use a NAT Gateway.</p>
<p>The screenshot below reflects the VPC stack built from the template mentioned above:<br><img src="/blog/indepth-study/vpc.png" alt="vpc"></p>
<h3 id="2-3-2-Create-a-Kafka-cluster-on-AWS-MSK-service"><a href="#2-3-2-Create-a-Kafka-cluster-on-AWS-MSK-service" class="headerlink" title="2.3.2. Create a Kafka cluster on AWS MSK service"></a>2.3.2. Create a Kafka cluster on AWS MSK service</h3><p>Under AWS MSK, I create a Kafka cluster <code>kafka-msk</code> under the VPC created in the above step and choose 2 for Number of Availability Zones. For these two Availability Zones in the VPC, choose the private subnets for each. Also set 1 broker per Availability Zone. The screenshot below reflects the configuration of the cluster:<br><img src="/blog/indepth-study/msk_config.png" alt="msk"></p>
<h3 id="2-3-3-Create-an-EC2-instance-and-connect-to-the-Kafka-cluster"><a href="#2-3-3-Create-an-EC2-instance-and-connect-to-the-Kafka-cluster" class="headerlink" title="2.3.3. Create an EC2 instance and connect to the Kafka cluster"></a>2.3.3. Create an EC2 instance and connect to the Kafka cluster</h3><p>To access the Kafka cluster, I create an EC2 instance as a client machine under the same VPC as the cluster. From local Terminal, connect to the EC2 instance and set up the Kafka environment with the following code:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Install Java </span>
sudo yum install java<span class="token number">-1.8</span><span class="token punctuation">.</span><span class="token number">0</span>

<span class="token comment" spellcheck="true"># Download Apache Kafka</span>
wget https<span class="token punctuation">:</span><span class="token operator">//</span>archive<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>org<span class="token operator">/</span>dist<span class="token operator">/</span>kafka<span class="token operator">/</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">/</span>kafka_2<span class="token number">.13</span><span class="token operator">-</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0.</span>tgz

<span class="token comment" spellcheck="true"># Run the following command in the directory where downloaded the TAR file in the previous step</span>
tar <span class="token operator">-</span>xzf kafka_2<span class="token number">.13</span><span class="token operator">-</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0.</span>tgz

<span class="token comment" spellcheck="true"># Rename the kafka_2.13-2.6.0 folder as kafka</span>
mv kafka_2<span class="token number">.13</span><span class="token operator">-</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0</span> kafka

<span class="token comment" spellcheck="true"># Go to the kafka folder</span>
cd kafka<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>kafka</code> folder contains the environment library to run the Kafka functions. To connect to the cluster <code>kafka-msk</code> via Bootstrap server or Zookeeper connection, first need to retrieve the client information by clicking the <code>View client information</code> on the <code>kafka-msk</code> cluster Details page, which will give the following information:<br><img src="/blog/indepth-study/msk_client_info.png" alt="msk_client"></p>
<p>Copy the TLS host/port pairs information under the Bootstrap servers:</p>
<pre><code>b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094</code></pre>
<p>Copy the Plaintext host/port pairs information under the ZooKeeper connection:</p>
<pre><code>z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181</code></pre>
<p>In the EC2 instance under <code>kafka</code> directory, run the following code to create a Kafka topic (also named as <code>quickstart</code>)</p>
<pre><code>bin/kafka-topics.sh --create --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --replication-factor 1 --partitions 1 --topic quickstart</code></pre>
<p>Then, run the describe command to make sure the topic is created successfully.</p>
<pre><code>bin/kafka-topics.sh --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --describe --topic quickstart</code></pre>
<p>The following screenshot reflects the existence of topic <code>quickstart</code>:<br><img src="/blog/indepth-study/msk_topic.png" alt="topic-quickstart"></p>
<p>To delete the topic just created, following code do that job:</p>
<pre><code>bin/kafka-topics.sh --delete --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --topic quickstart</code></pre>
<p>To talk to the MSK cluster, we need to use the JVM truststore. To do this, first create a folder named <code>/tmp</code> on the client machine. Then, go to the <code>bin</code> folder of the Apache Kafka installation and run the following command:</p>
<pre><code>cp /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.amzn2.0.1.x86_64/jre/lib/security/cacerts /tmp/kafka.client.truststore.jks</code></pre>
<p>While still in the <code>bin</code> folder of the Apache Kafka installation on the client machine, create a text file named <code>client.properties</code> with the following contents:</p>
<pre><code>security.protocol=SSL
ssl.truststore.location=/tmp/kafka.client.truststore.jks</code></pre>
<h3 id="2-3-4-Produce-and-consume-messages-from-the-topic"><a href="#2-3-4-Produce-and-consume-messages-from-the-topic" class="headerlink" title="2.3.4. Produce and consume messages from the topic"></a>2.3.4. Produce and consume messages from the topic</h3><h4 id="2-3-4-1-Produce-messages-to-the-created-topic"><a href="#2-3-4-1-Produce-messages-to-the-created-topic" class="headerlink" title="2.3.4.1. Produce messages to the created topic"></a>2.3.4.1. Produce messages to the created topic</h4><p><strong>Go to the <code>bin</code> folder first.</strong> Similarly, initialize the Kafka producer console, but listen to the cluster <code>kafka-msk</code> in MSK this time using Bootstrap server at topic <code>quickstart</code>. And then input the same three messages:</p>
<pre><code>./kafka-console-producer.sh --bootstrap-server b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094 --producer.config client.properties --topic quickstart
&gt;This is the first event.
&gt;This is the second event.
&gt;This is the third event.</code></pre>
<p>The screenshot below reflects the process of producing messages:<br><img src="/blog/indepth-study/msk_producer.png" alt="msk_producer"></p>
<h4 id="2-3-4-2-Consume-the-messages-produced-by-the-producer"><a href="#2-3-4-2-Consume-the-messages-produced-by-the-producer" class="headerlink" title="2.3.4.2. Consume the messages produced by the producer"></a>2.3.4.2. Consume the messages produced by the producer</h4><p>Still stay in the <code>bin</code> folder. Similarly, initialize the Kafka consumer console, and listen to the cluster <code>kafka-msk</code> at topic <code>quickstart</code> from beginning:</p>
<pre><code>./kafka-console-consumer.sh --bootstrap-server b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094 --consumer.config client.properties --from-beginning --topic quickstart</code></pre>
<p>The screenshot below reflects that the consumer successfully received the messages:<br><img src="/blog/indepth-study/msk_consumer.png" alt="msk_comsumer"></p>
<h3 id="2-3-5-Using-Amazon-MSK-as-an-event-source-for-AWS-Lambda"><a href="#2-3-5-Using-Amazon-MSK-as-an-event-source-for-AWS-Lambda" class="headerlink" title="2.3.5. Using Amazon MSK as an event source for AWS Lambda"></a>2.3.5. Using Amazon MSK as an event source for AWS Lambda</h3><p>Follow <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/compute/using-amazon-msk-as-an-event-source-for-aws-lambda/">this blog post</a> for most parts of configuration to allow a Lambda function to be triggered by new messages in a MSK cluster.</p>
<h4 id="2-3-5-1-Required-Lambda-function-permissions"><a href="#2-3-5-1-Required-Lambda-function-permissions" class="headerlink" title="2.3.5.1. Required Lambda function permissions"></a>2.3.5.1. Required Lambda function permissions</h4><p>The Lambda must have permission to describe VPCs and security groups, and manage elastic network interfaces. To access the Amazon MSK data stream, the Lambda function also needs two Kafka permissions: kafka:DescribeCluster and kafka:GetBootstrapBrokers. The policy template <code>AWSLambdaMSKExecutionRole</code> includes these permissions.</p>
<p>Therefore, I create a role in IAM as <code>msk_lambda</code> and attach the policy template <code>AWSLambdaMSKExecutionRole</code> with it for later use as the Execution role for the Lambda function.</p>
<h4 id="2-3-5-2-Configure-the-Lambda-event-source-mapping"><a href="#2-3-5-2-Configure-the-Lambda-event-source-mapping" class="headerlink" title="2.3.5.2. Configure the Lambda event source mapping"></a>2.3.5.2. Configure the Lambda event source mapping</h4><p>First, I create a Lambda function <code>Kafka_Lambda</code> with runtime as Python 3.7. Meanwhile, make sure to attach the function under the same VPC and two private subnets as shown in the screenshot below:<br><img src="/blog/indepth-study/lambda_vpc.png" alt="lambda_vpc"></p>
<p>Next, in the Terminal window that connected to the <code>kafka-msk</code> cluster, create a new topic as <code>kafka-lambda</code> using:</p>
<pre><code>bin/kafka-topics.sh --create --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --replication-factor 1 --partitions 1 --topic kafka-msk</code></pre>
<p>Under the Lambda function <code>Kafka_Lambda</code> Configuration tab, add MSK cluster <code>kafka-msk</code>‘s topic <code>kafka-lambda</code> as the event source as shown below:<br><img src="/blog/indepth-study/lambda_trigger.png" alt="trigger"></p>
<h4 id="2-3-5-3-Produce-a-message-in-the-topic-to-trigger-the-Lambda-function"><a href="#2-3-5-3-Produce-a-message-in-the-topic-to-trigger-the-Lambda-function" class="headerlink" title="2.3.5.3. Produce a message in the topic to trigger the Lambda function"></a>2.3.5.3. Produce a message in the topic to trigger the Lambda function</h4><p>Connect to the EC2 instance and under the <code>bin</code> folder of the kafka environment, initialize the Kafka producer console and produce a message in the topic <code>kafka-lambda</code>:</p>
<pre><code>./kafka-console-producer.sh --bootstrap-server b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094 --producer.config client.properties --topic kafka-lambda
&gt;This is a test message to trigger Lambda function.</code></pre>
<p>Then, back to the Lambda function and choose to view logs in CloudWatch. Select the latest log stream gives:<br><img src="/blog/indepth-study/log.png" alt="log"></p>
<p>From the screenshot above, it is clear that the Lambda function was successfully triggered by the MSK cluster. The Lambda function’s event payload contains an array of records. Each array item contains details of the topic and Kafka partition identifier, together with a timestamp and base64 encoded message. Following code deployed under the Lambda function print out the <code>event</code> and <code>records</code> content, also decode the base64 encoded message that under the <code>value</code> attribute.</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> json
<span class="token keyword">import</span> base64

<span class="token keyword">def</span> <span class="token function">lambda_handler</span><span class="token punctuation">(</span>event<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span>
    record <span class="token operator">=</span> event<span class="token punctuation">[</span><span class="token string">'records'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'kafka-lambda-0'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>record<span class="token punctuation">)</span>
    msg <span class="token operator">=</span> record<span class="token punctuation">[</span><span class="token string">'value'</span><span class="token punctuation">]</span>
    msg_bytes <span class="token operator">=</span> base64<span class="token punctuation">.</span>b64decode<span class="token punctuation">(</span>msg<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'ascii'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    message <span class="token operator">=</span> msg_bytes<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'ascii'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>message<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="3-Task-3-Spark-Setup"><a href="#3-Task-3-Spark-Setup" class="headerlink" title="3. Task 3: Spark Setup"></a>3. Task 3: Spark Setup</h1><h2 id="3-1-Spark-Installation"><a href="#3-1-Spark-Installation" class="headerlink" title="3.1. Spark Installation"></a>3.1. Spark Installation</h2><p>Working on Mac, I choose to use Homebrew for installation:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Install Scala</span>
brew install scala

<span class="token comment" spellcheck="true"># Install Spark</span>
brew install apache<span class="token operator">-</span>spark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>The Spark environment will be installed under <code>/usr/local/Cellar/apache-spark/3.0.1</code>. And run <code>spark-shell</code> in the terminal to start the Spark server:<br><img src="/blog/indepth-study/spark-shell.png" alt="spark-shell"></p>
<p>It is also available to start the Python Spark API server <code>PySpark</code> with <code>pyspark</code>:<br><img src="/blog/indepth-study/pyspark.png" alt="pyspark"></p>
<p>Keep the Spark server running, Spark user interface can be accessed through the link: <a target="_blank" rel="noopener" href="https://localhost:4040/jobs/">https://localhost:4040/jobs/</a>. Run the following Spark job in the Terminal shell, the UI would display the related job information:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> pyspark
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token operator">>></span><span class="token operator">></span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> rdd<span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/blog/indepth-study/spark_job.png" alt="spark_job"></p>
<p>To manually start any other Spark server, can go to the directory <code>/usr/local/Cellar/apache-spark/3.0.1/libexec/sbin</code> and run the function list in the <code>sbin</code> folder:<br><img src="/blog/indepth-study/spark_server.png" alt="spark_server"></p>
<p>For example, under the <code>sbin</code> folder, run <code>./start-master.sh</code> to start the Master server and the Master server UI can be accessed through the link: <a target="_blank" rel="noopener" href="https://localhost:8080/">https://localhost:8080/</a> as shown below:<br><img src="/blog/indepth-study/spark_master.png" alt="spark_master"></p>
<h2 id="3-2-Working-with-AWS-EMR"><a href="#3-2-Working-with-AWS-EMR" class="headerlink" title="3.2. Working with AWS EMR"></a>3.2. Working with AWS EMR</h2><h3 id="3-2-1-Create-a-Spark-cluster-on-AWS-EMR"><a href="#3-2-1-Create-a-Spark-cluster-on-AWS-EMR" class="headerlink" title="3.2.1. Create a Spark cluster on AWS EMR"></a>3.2.1. Create a Spark cluster on AWS EMR</h3><p>Under AWS EMR, I create a Spark cluster <code>spark-emr</code> with the configuration showed in the screenshot below:<br><img src="/blog/indepth-study/emr.png" alt="emr_spec"></p>
<h3 id="3-2-2-Working-with-EMR-Notebooks"><a href="#3-2-2-Working-with-EMR-Notebooks" class="headerlink" title="3.2.2. Working with EMR Notebooks"></a>3.2.2. Working with EMR Notebooks</h3><p>As mentioned in the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html">AWS EMR documentation</a>, the user can use Amazon EMR Notebooks along with Amazon EMR clusters running Apache Spark to create and open Jupyter Notebook and JupyterLab interfaces within the Amazon EMR console.</p>
<p>In the EMR Notebooks panel, I start a notebook attached to the cluster created in the above step (make sure the cluster installs the application JupyterEnterpriseGateway). Then open the notebook under the PySpark environment and run some test code. The screenshot below reflects the success of the Spark job processing.<br><img src="/blog/indepth-study/emr_notebook.png" alt="emr_notebook"></p>
<h3 id="3-2-3-Submit-a-PySpark-job-to-the-EMR-cluster"><a href="#3-2-3-Submit-a-PySpark-job-to-the-EMR-cluster" class="headerlink" title="3.2.3. Submit a PySpark job to the EMR cluster"></a>3.2.3. Submit a PySpark job to the EMR cluster</h3><p>The main idea for this part is to build a simple spark job and execute it as a step in the EMR cluster.</p>
<h4 id="3-2-3-1-Create-the-job-script-and-save-it-in-the-S3-bucket"><a href="#3-2-3-1-Create-the-job-script-and-save-it-in-the-S3-bucket" class="headerlink" title="3.2.3.1. Create the job script and save it in the S3 bucket"></a>3.2.3.1. Create the job script and save it in the S3 bucket</h4><p>The following python script is a sample Word Count job that was used later. I upload it into the S3 bucket with the path <code>s3://pyspark-job/script/pyspark_word_count.py</code>.</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">import</span> sys
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> len<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Usage: wordcount "</span><span class="token punctuation">,</span> file<span class="token operator">=</span>sys<span class="token punctuation">.</span>stderr<span class="token punctuation">)</span>
        exit<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>appName<span class="token operator">=</span><span class="token string">"demo-word-count"</span><span class="token punctuation">)</span>
    text_file <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    counts <span class="token operator">=</span> text_file<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>
    counts<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    sc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="3-2-3-2-Create-a-input-text-and-save-it-in-the-S3-bucket"><a href="#3-2-3-2-Create-a-input-text-and-save-it-in-the-S3-bucket" class="headerlink" title="3.2.3.2. Create a input text and save it in the S3 bucket"></a>3.2.3.2. Create a input text and save it in the S3 bucket</h4><p>I generate a 10 paragraphs 1000 words <a href="/blog/indepth-study/text.txt">Lorem Ipusm text file</a> as the input for the Word Count job and upload it into the S3 bucket with the path <code>s3://pyspark-job/input/text.txt</code>.</p>
<h4 id="3-2-3-3-Submit-the-job-as-a-step-to-the-cluster"><a href="#3-2-3-3-Submit-the-job-as-a-step-to-the-cluster" class="headerlink" title="3.2.3.3. Submit the job as a step to the cluster"></a>3.2.3.3. Submit the job as a step to the cluster</h4><p>To submit the PySpark job as a step to the EMR cluster and then execute in it, and save the result as a output file in the S3 bucket, following code do that job:</p>
<pre><code>aws emr add-steps --cluster-id [Cluster-ID] \
--steps Type=spark,Name=[Name-Your-Step],\
Args=[--deploy-mode,cluster,--master,yarn,\
--conf,spark.yarn.submit.waitAppCompletion=false,\
--num-executors,2,--executor-cores,2,--executor-memory,1g,\
[Path-To-Script-In-S3],\
[Path-To-Input-In-S3],\
[Path-To-Output-In-S3]],\
ActionOnFailure=CONTINUE</code></pre>
<p>Replace all the placeholders with the related values and run the command line:</p>
<pre><code>aws emr add-steps --cluster-id &quot;j-4CN8X8R8ZOSL&quot; \
--steps Type=spark,Name=spark-job-step,\
Args=[--deploy-mode,cluster,--master,yarn,\
--conf,spark.yarn.submit.waitAppCompletion=false,\
s3://pyspark-job/script/pyspark_word_count.py,\
s3://pyspark-job/input/text.txt,\
s3://pyspark-job/output/],\
ActionOnFailure=CONTINUE</code></pre>
<p><em>You may need to run <code>setopt +o nomatch</code> first if you are running the zsh shell in your Terminal to avoid any <code>zsh: no match found</code> error.</em></p>
<p>After successfully execute the command, the StepId will be returned:</p>
<pre><code>&#123;
    &quot;StepIds&quot;: [
        &quot;s-8YATQIVZWW7X&quot;
    ]
&#125;</code></pre>
<p>And then under the EMR cluster <strong>Steps</strong> tab, step <code>spark-job-step</code> status will turn to <strong>Completed</strong> after few seconds as shown below:<br><img src="/blog/indepth-study/msk_step.png" alt="step"></p>
<p>Meanwhile, under the <code>output</code> folder in the S3 bucket, step results are saved as separate part files which store the cout number for each word in the input file:<br><img src="/blog/indepth-study/msk_step_output.png" alt="step_output"></p>
<h1 id="4-Task-4-Real-time-Credit-Card-Fraud-Detection-Pipeline"><a href="#4-Task-4-Real-time-Credit-Card-Fraud-Detection-Pipeline" class="headerlink" title="4. Task 4: Real-time Credit Card Fraud Detection Pipeline"></a>4. Task 4: Real-time Credit Card Fraud Detection Pipeline</h1><h2 id="4-1-Workflow-and-Architecture"><a href="#4-1-Workflow-and-Architecture" class="headerlink" title="4.1. Workflow and Architecture"></a>4.1. Workflow and Architecture</h2><p>Following figure illustrates the workflow and architecture of the whole pipeline:<br><img src="/blog/indepth-study/architecture.png" alt="architecture"></p>
<p>For the set-up part, I first simulate 100 customers’ information and stored in the <a href="/blog/indepth-study/customer.txt"><code>customer.csv</code></a> file, and over 10K transaction records stored in the <code>transaction_training.csv</code> file. Then, call a Spark SQL job to retrieve those data and import them into a Cassandra database. Next, run a Spark ML job to read the data from Cassandra, train on those data and create the models (Preprocessing and Random Forest) to classify the transaction records are fraud or not. </p>
<p>After the models are saved to the file system, start a Spark Streaming job that would load the ML models and also consume credit card transactions from Kafka. Create a Kafka topic and produce transaction records from the <code>transaction_testing.csv</code> file as messages that would be consumed by the Spark Streaming job. And the streaming job would predict whether these transactions are fraud or not and then save them into the <code>fraud_transaction</code> and <code>non_fraud_transaction</code> tables separately based on the classification.</p>
<p>With the classified in-coming transaction records stored in the Cassandra database, I use the Spring Boot framework to display the frand and non-fraud transactions in real-time on the dashboard web page. Meanwhile, I also use the Flask framework to create two REST APIs that could easily retrieve the customer’s information and create the transaction statement for each customer.</p>
<p><strong>A video demo for the workflow could be found here:</strong> <a target="_blank" rel="noopener" href="https://youtu.be/fOVsxk16b0w">https://youtu.be/fOVsxk16b0w</a></p>
<h2 id="4-2-Implementation-Details"><a href="#4-2-Implementation-Details" class="headerlink" title="4.2. Implementation Details"></a>4.2. Implementation Details</h2><h3 id="4-2-1-Customers-amp-Transactions-dataset"><a href="#4-2-1-Customers-amp-Transactions-dataset" class="headerlink" title="4.2.1. Customers &amp; Transactions dataset"></a>4.2.1. Customers &amp; Transactions dataset</h3><p>Stimulate 100 customers using <a target="_blank" rel="noopener" href="https://www.mockaroo.com/">Mockaroo</a>. For each record, it includes following columns (information):</p>
<ul>
<li>cc_num: credit card number which uniquely identify each card / customer</li>
<li>first: customer’s first name</li>
<li>last: customer’s last name</li>
<li>gender: customer’s gender</li>
<li>street</li>
<li>city</li>
<li>state</li>
<li>zip: zip code for the address above</li>
<li>lat: latitude for the address above</li>
<li>long: longitude for the address above</li>
<li>job: customer’s vocation</li>
<li>dob: the date of birth for the customer</li>
</ul>
<p>Also generate over 10K transaction records for these customers using the same way. For each record, it includes following columns (information):</p>
<ul>
<li>cc_num: credit card number which uniquely identify each card / customer</li>
<li>first: customer’s first name</li>
<li>last: customer’s last name</li>
<li>trans_num: transaction number</li>
<li>trans_date: transaction date</li>
<li>trans_time: transaction time</li>
<li>unix_time: transaction time in unix timestamp format</li>
<li>category: category for the purchased item</li>
<li>amt: transaction amount</li>
<li>merchant: the place that the transaction happened</li>
<li>merch_lat: latitude for the merchant</li>
<li>merch_long: longitude for the merchant</li>
<li>is_fraud: boolean to indicate the transaction is fraud or not</li>
</ul>
<p>These transaction records would be later used as the training set and testing set with the splitting ratio as 80%.</p>
<h3 id="4-2-2-Spark-ML-job"><a href="#4-2-2-Spark-ML-job" class="headerlink" title="4.2.2. Spark ML job"></a>4.2.2. Spark ML job</h3><p>First, run a Spark SQL job to retrieve the customers and transaction training data and import them into the Cassandra database. When importing the transactions data, the job also calculates two extra features <code>age</code> and <code>distance</code> where <code>age</code> is the age of each customer by the time the data are imported according to his/her date of birth; <code>distance</code> is the distance between the customer’s address and the merchant’s address by calculating the Euclidean distance between two places using the latitude &amp; longitude information. All the training data (including two extra features) would be splitted and stored in the fraud table and non-fraud table separately based on whether each record is fraud or not.</p>
<p>Spark ML Job will load fraud and non-fraud transactions from fraud and non-fraud tables respectively. This will create 2 different dataframes in Spark. Next, these 2 dataframes are combined together using Union function, and Spark ML Pipeline Stages will be applied on this dataframe. </p>
<ul>
<li>First, String Indexer will be applied to transform the selected columns into double values. Since the machine learning algorithm would not understand string values but only double values. </li>
<li>Second, One Hot Encoder will be applied to normalize these double values. The reason double values must be normalised is because machine-learning algorithm assumes higher the value better the category.</li>
<li>Third, Vector Assembler is applied to assemble all the transformed columns into one column. This column is called a feature column. And the values of this column is a vector. This feature column will be given as input to the model creation algorithm. </li>
</ul>
<p>After assembling the feature column, then train the algorithm with this dataframe. However, currently data are not balanced, given that the number of non-fraud transactions is way larger than the number of fraud transactions. If such unbalanced data are used for training, then the algorithm would not create an accurate model. Hence, there is one more step needed to apply before training is to balance the data to enforce the number of non-fraud transactions must be almost equal to the number of fraud transactions. Here, the job uses the K-means algorithm to reduce the number of non-fraud transactions. After that, the job will combine both the dataframes and form a single dataframe as a balanced one. Then apply the Random Forest algorithm on this dataframe that uses the feature column for training and create the prediction/classification model. And finally, save the model to the filesystem.</p>
<p>Following figure illustrates the entire Spark ML job workflow:<br><img src="/blog/indepth-study/spark_ml.png" alt="spark_ml"></p>
<h3 id="4-2-3-Kafka-producer"><a href="#4-2-3-Kafka-producer" class="headerlink" title="4.2.3. Kafka producer"></a>4.2.3. Kafka producer</h3><p>Create a Kafka topic named as <code>creditcardTransaction</code> with 3 partitions. </p>
<pre><code>kafka-topics --zookeeper localhost:2181 --create --topic creditcardTransaction  --replication-factor 1 --partitions 3</code></pre>
<p>The Kafka producer job would randomly select transactions from the transaction training dataset as messages and save the current timestamp into the messages as the transaction time. Later, these messages would be fed into the Spark Streaming job.</p>
<h3 id="4-2-4-Spark-Streaming-job"><a href="#4-2-4-Spark-Streaming-job" class="headerlink" title="4.2.4. Spark Streaming job"></a>4.2.4. Spark Streaming job</h3><p>In the Spark Streaming job, it first starts by consuming credit card transaction messages from Kafka via the topic <code>creditcardTransaction</code>. Then, for each message, it reads customer data from Cassandra to compute age of the customer and calculate distance between merchant and customer place, since age and distance will be used as features in the prediction. After that, load both Preprocessing and Random Forest models that were created by Spark ML job. These 2 models will be used to transform and predict whether a transaction is fraud or not. Once the transactions are predicted, the records would be saved in the Cassandra database where fraud transactions will be saved to the fraud table and non-fraud transactions will be saved to the non-fraud table. Also each message would be offered a partition number and an offset number to indicate the location in the topic. These partition and offset information would be saved in the Kafka offset table to help to achieve exactly once semantics.</p>
<h3 id="4-2-5-Front-end-dashboard"><a href="#4-2-5-Front-end-dashboard" class="headerlink" title="4.2.5. Front-end dashboard"></a>4.2.5. Front-end dashboard</h3><p>The front-end dashboard class is designed with Spring Bot framework that would select fraud and non-fraud transactions from Cassandra tables and display it on the dashboard in real-time. This method will call a select query to retrieve the latest fraud and non-fraud transactions that occurred in the last 5 seconds and display it on the dashboard. To display the record only once, the method maintains the max timestamp of previously displayed fraud/non-fraud transactions. And in the current trigger, it would only select those transactions whose timestamp is greater than the previous max timestamp.</p>
<p>Following screenshot illustrates a basic scenario of the dashboard interface:<br><img src="/blog/indepth-study/dashboard.png" alt="dashboard"></p>
<h3 id="4-2-6-REST-API-for-customers-and-transaction-statements"><a href="#4-2-6-REST-API-for-customers-and-transaction-statements" class="headerlink" title="4.2.6. REST API for customers and transaction statements"></a>4.2.6. REST API for customers and transaction statements</h3><p>I also design two REST APIs with the Flask framework to easily retrieve the customer information and create transaction statements for customers. They are all implemented by calling SQL queries to select records from the Cassandra non-fraud table.</p>
<ul>
<li>For customer information, the endpoint is: <code>/api/customer/&lt;cc_num&gt;</code> which would return basic information for the credit card <code>&lt;cc_num&gt;</code> owner.</li>
<li>For creating a transaction statement for the specific customer, the endpoint is: <code>api/statement/&lt;cc_num&gt;</code> which would return all the transaction records for the credit card <code>&lt;cc_num&gt;</code> and order them by transaction time.</li>
</ul>
<p>Following screenshots illustrate the examples of two API calls:<br><img src="/blog/indepth-study/api_customer.png" alt="api_customer"></p>
<p><img src="/blog/indepth-study/api_statement.png" alt="api_statement"></p>
</p></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/blog/something-of-value/" title="Something of Value by Howard Marks"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: Something of Value by Howard Marks</a></span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/blog/matrix-factorization/" title="Decode the Secrets Behind Recommender Systems">Next post: Decode the Secrets Behind Recommender Systems&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2021&nbsp;<a target="_blank" href="https://j-an.org/" rel="noopener noreferrer">J. An</a></p></footer></div></div></div></div><script src="/blog/js/jquery-3.1.0.min.js"></script><script src="/blog/js/bootstrap.min.js"></script><script src="/blog/js/jquery-migrate-1.2.1.min.js"></script><script src="/blog/js/jquery.appear.js"></script><script src="/blog/js/google-analytics.js"></script><script src="/blog/js/typography.js"></script></body></html>