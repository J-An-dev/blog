<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>In-depth Study · J. An's Blog</title><meta name="description" content="The in-depth study documentation for the graduate level course COMS 6998: Cloud Computing &amp;amp; Big Data taken in Fall 2020 @ Columbia. Topics include"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/blog/css/bootstrap.min.css"><link rel="stylesheet" href="/blog/css/font-awesome.min.css"><link rel="stylesheet" href="/blog/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        }
    })

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><meta name="generator" content="Hexo 5.2.0"><link rel="stylesheet" href="/blog/css/prism-base16-ateliersulphurpool.light.css" type="text/css">
<link rel="stylesheet" href="/blog/css/prism-line-numbers.css" type="text/css"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/blog/" class="a-title"></a></h3><h1 tabindex="-1" class="site-title-large"><a href="/blog/" class="a-title">J. An's Blog</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/blog/">Home</a></li><li><a href="/blog/archives">Archive</a></li><li><a href="/blog/tags">Tags</a></li><li class="soc"><a href="https://github.com/j-an-dev" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a><a href="https://twitter.com/anjie_tweet" target="_blank" rel="noopener noreferrer"><i class="fa fa-twitter">&nbsp;</i></a><a href="https://www.instagram.com/aj_ins" target="_blank" rel="noopener noreferrer"><i class="fa fa-instagram">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2020&nbsp;<a target="_blank" href="https://j-an.org/" rel="noopener noreferrer">J. An</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>In-depth Study</a></p><p class="post-meta"><span class="date meta-item">Posted on&nbsp;2020-12-01</span></p><p class="post-abstract"><p>The in-depth study documentation for the graduate level course COMS 6998: Cloud Computing &amp; Big Data taken in Fall 2020 @ Columbia. Topics include the ELK Stack, Kafka (w/ AWS MSK) and more to be determined.<a id="more"></a></p>
</br>

<p>Table of Contents <!-- omit in toc --></p>
<ul>
<li><a href="#1-task-1-elk-stack">1. Task 1: ELK Stack</a><ul>
<li><a href="#11-elk-stack-installation">1.1. ELK Stack Installation</a></li>
<li><a href="#12-example-of-a-logstash-pipeline-sending-syslog-logs-into-the-stack">1.2. Example of a Logstash pipeline sending syslog logs into the Stack</a><ul>
<li><a href="#121-implementation-details">1.2.1. Implementation details</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-task-2-kafka-pipeline">2. Task 2: Kafka Pipeline</a><ul>
<li><a href="#21-kafka-installation">2.1. Kafka Installation</a></li>
<li><a href="#22-example-of-a-kafka-messages-producing-and-consuming-pipeline">2.2. Example of a Kafka messages producing and consuming pipeline</a><ul>
<li><a href="#221-create-a-kafka-topic-to-store-events">2.2.1. Create a Kafka topic to store events</a></li>
<li><a href="#222-produce-messages-to-the-created-topic">2.2.2. Produce messages to the created topic</a></li>
<li><a href="#223-consume-the-messages-produced-by-the-producer">2.2.3. Consume the messages produced by the producer</a></li>
</ul>
</li>
<li><a href="#23-create-a-kafka-pipeline-on-aws">2.3. Create a Kafka pipeline on AWS</a><ul>
<li><a href="#231-vpc-network-configuration">2.3.1. VPC network configuration</a></li>
<li><a href="#232-create-a-kafka-cluster-on-aws-msk-service">2.3.2. Create a Kafka cluster on AWS MSK service</a></li>
<li><a href="#233-create-an-ec2-instance-and-connect-to-the-kafka-cluster">2.3.3. Create an EC2 instance and connect to the Kafka cluster</a></li>
<li><a href="#234-produce-and-consume-messages-from-the-topic">2.3.4. Produce and consume messages from the topic</a><ul>
<li><a href="#2341-produce-messages-to-the-created-topic">2.3.4.1. Produce messages to the created topic</a></li>
<li><a href="#2342-consume-the-messages-produced-by-the-producer">2.3.4.2. Consume the messages produced by the producer</a></li>
</ul>
</li>
<li><a href="#235-using-amazon-msk-as-an-event-source-for-aws-lambda">2.3.5. Using Amazon MSK as an event source for AWS Lambda</a><ul>
<li><a href="#2351-required-lambda-function-permissions">2.3.5.1. Required Lambda function permissions</a></li>
<li><a href="#2352-configure-the-lambda-event-source-mapping">2.3.5.2. Configure the Lambda event source mapping</a></li>
<li><a href="#2353-produce-a-message-in-the-topic-to-trigger-the-lambda-function">2.3.5.3. Produce a message in the topic to trigger the Lambda function</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</br>

<h1 id="1-Task-1-ELK-Stack"><a href="#1-Task-1-ELK-Stack" class="headerlink" title="1. Task 1: ELK Stack"></a>1. Task 1: ELK Stack</h1><h2 id="1-1-ELK-Stack-Installation"><a href="#1-1-ELK-Stack-Installation" class="headerlink" title="1.1. ELK Stack Installation"></a>1.1. ELK Stack Installation</h2><p>I followed the tutorial <a target="_blank" rel="noopener" href="https://logz.io/learn/complete-guide-elk-stack/">The Complete Guide to the ELK Stack</a> to digest the basic concepts of th ELK Stack and tutorial <a target="_blank" rel="noopener" href="https://logz.io/blog/elk-mac/">Installing the ELK Stack on Mac OS X</a> to install the required services using Homebrew. The screenshot below reflects the success of installation.</p>
<p><img src="/blog/indepth-study/installation.png" alt="installation"></p>
<h2 id="1-2-Example-of-a-Logstash-pipeline-sending-syslog-logs-into-the-Stack"><a href="#1-2-Example-of-a-Logstash-pipeline-sending-syslog-logs-into-the-Stack" class="headerlink" title="1.2. Example of a Logstash pipeline sending syslog logs into the Stack"></a>1.2. Example of a Logstash pipeline sending syslog logs into the Stack</h2><p>In this example, I implemented a Logstash pipeline that collecting system log data and shipping them into the Stack. Then, created the index pattern <code>syslog-demo</code> in Kibana to visualize the syslog data.</p>
<h3 id="1-2-1-Implementation-details"><a href="#1-2-1-Implementation-details" class="headerlink" title="1.2.1. Implementation details"></a>1.2.1. Implementation details</h3><ol>
<li><p>Start services Elasticsearch and Kibana</p>
<pre><code>brew services start elasticsearch
brew services start kibana</code></pre>
</li>
<li><p>Create a new Logstash configuration file</p>
<pre><code># Make the directory
sudo mkdir -p /etc/logstash/conf.d
# Locate to the configuration folder
cd /etc/logstash/conf.d
# Create the conf file
sudo vim  /etc/logstash/conf.d/syslog.conf</code></pre>
<p>Enter the following configuration in the <code>syslog.conf</code> file:</p>
<pre><code>input &#123;
    file &#123;
        path =&gt; [ &quot;/var/log/*.log&quot;, &quot;/var/log/messages&quot;, &quot;/var/log/syslog&quot; ]
        type =&gt; &quot;syslog&quot;
    &#125;
&#125;

filter &#123;
    if [type] == &quot;syslog&quot; &#123;
        grok &#123;
        match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGTIMESTAMP:syslog_timestamp&#125; %&#123;SYSLOGHOST:syslog_hostname&#125; %&#123;DATA:syslog_program&#125;(?:\[%&#123;POSINT:syslog_pid&#125;\])?: %&#123;GREEDYDATA:syslog_message&#125;&quot; &#125;
        add_field =&gt; [ &quot;received_at&quot;, &quot;%&#123;@timestamp&#125;&quot; ]
        add_field =&gt; [ &quot;received_from&quot;, &quot;%&#123;host&#125;&quot; ]
        &#125;
        syslog_pri &#123; &#125;
        date &#123;
        match =&gt; [ &quot;syslog_timestamp&quot;, &quot;MMM  d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ]
        &#125;
    &#125;
&#125;

output &#123;
    elasticsearch &#123;
        hosts =&gt; [&quot;127.0.0.1:9200&quot;] 
        index =&gt; &quot;syslog-demo&quot;
    &#125;
    stdout &#123; codec =&gt; rubydebug &#125;
&#125;</code></pre>
</li>
<li><p>Start the Logstash pipeline</p>
</li>
</ol>
<p>Make sure under the directory <code>/etc/logstash/conf.d/</code>, then run <code>logstash -f syslog.conf</code> to start the pipeline giving the following logs:<br><img src="/blog/indepth-study/logstash_pipeline.png" alt="Logstach Pipeline"></p>
<p>In Kibana (by accessing <a target="_blank" rel="noopener" href="https://localhost:5601/">https://localhost:5601</a>), initialized an index pattern with the index name <code>syslog-demo</code> created by the Logstash pipeline and selected <code>@timestamp</code> field as the Time Filter field name.<br><img src="/blog/indepth-study/kibana_index_pattern.png" alt="Kibana Index Pattern"></p>
<p>Under the <code>Discover</code> page, syslog data could be found:<br><img src="/blog/indepth-study/discover_log.png" alt="Discover Logs"></p>
<h1 id="2-Task-2-Kafka-Pipeline"><a href="#2-Task-2-Kafka-Pipeline" class="headerlink" title="2. Task 2: Kafka Pipeline"></a>2. Task 2: Kafka Pipeline</h1><h2 id="2-1-Kafka-Installation"><a href="#2-1-Kafka-Installation" class="headerlink" title="2.1. Kafka Installation"></a>2.1. Kafka Installation</h2><p>Working on Mac, I choose to use Homebrew for installation:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># This will install Java 1.8, Kafka, ZooKeeper at the same time</span>
brew install kafka

<span class="token comment" spellcheck="true"># This will run ZooKeeper and Kafka as services</span>
brew services start zookeeper
brew services start kafka<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>The screenshot below reflects the success of installation.<br><img src="/blog/indepth-study/kafka_installation.png" alt="kafka_installation"></p>
<h2 id="2-2-Example-of-a-Kafka-messages-producing-and-consuming-pipeline"><a href="#2-2-Example-of-a-Kafka-messages-producing-and-consuming-pipeline" class="headerlink" title="2.2. Example of a Kafka messages producing and consuming pipeline"></a>2.2. Example of a Kafka messages producing and consuming pipeline</h2><h3 id="2-2-1-Create-a-Kafka-topic-to-store-events"><a href="#2-2-1-Create-a-Kafka-topic-to-store-events" class="headerlink" title="2.2.1. Create a Kafka topic to store events"></a>2.2.1. Create a Kafka topic to store events</h3><p>I name the topic as <code>quickstart</code>. And following code initialize that topic with 1 partition and 1 replication factor:</p>
<pre><code>kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic quickstart</code></pre>
<h3 id="2-2-2-Produce-messages-to-the-created-topic"><a href="#2-2-2-Produce-messages-to-the-created-topic" class="headerlink" title="2.2.2. Produce messages to the created topic"></a>2.2.2. Produce messages to the created topic</h3><p>Initialize the Kafka producer console, which will listen to localhost at port 9092 at topic <code>qucikstart</code>. And then input three messages:</p>
<pre><code>kafka-console-producer --bootstrap-server localhost:9092 --topic quickstart
&gt;This is the first event.
&gt;This is the second event.
&gt;This is the third event.</code></pre>
<h3 id="2-2-3-Consume-the-messages-produced-by-the-producer"><a href="#2-2-3-Consume-the-messages-produced-by-the-producer" class="headerlink" title="2.2.3. Consume the messages produced by the producer"></a>2.2.3. Consume the messages produced by the producer</h3><p>Open another Terminal window and initialize the Kafka consumer console, which will listen to bootstrap server localhost at port 9092 at topic <code>qucikstart</code> from beginning:</p>
<pre><code>kafka-console-consumer --bootstrap-server localhost:9092 --topic quickstart --from-beginning</code></pre>
<p>Thie screenshot below reflects that the consumer successfully received the messages:<br><img src="/blog/indepth-study/events.png" alt="events"></p>
<h2 id="2-3-Create-a-Kafka-pipeline-on-AWS"><a href="#2-3-Create-a-Kafka-pipeline-on-AWS" class="headerlink" title="2.3. Create a Kafka pipeline on AWS"></a>2.3. Create a Kafka pipeline on AWS</h2><h3 id="2-3-1-VPC-network-configuration"><a href="#2-3-1-VPC-network-configuration" class="headerlink" title="2.3.1. VPC network configuration"></a>2.3.1. VPC network configuration</h3><p>Amazon MSK is a highly available service, so it must be configured to run in a minimum of two Availability Zones in the preferred Region. To comply with security best practice, the brokers are usually configured in private subnets in each Region.</p>
<p>For later one Amazon MSK to invoke Lambda, must ensure that there is a NAT Gateway running in the public subnet of each Region. I configure a new VPC with public and private subnets in two Availability Zones using <a target="_blank" rel="noopener" href="https://gist.github.com/jbesw/f9401b4c52a7446ef1bb71ceea8cc3e8">this</a> AWS CloudFormation template. In this configuration, the public subnets are set up to use a NAT Gateway.</p>
<p>The screenshot below reflets the VPC stack built from the template mentioned above:<br><img src="/blog/indepth-study/vpc.png" alt="vpc"></p>
<h3 id="2-3-2-Create-a-Kafka-cluster-on-AWS-MSK-service"><a href="#2-3-2-Create-a-Kafka-cluster-on-AWS-MSK-service" class="headerlink" title="2.3.2. Create a Kafka cluster on AWS MSK service"></a>2.3.2. Create a Kafka cluster on AWS MSK service</h3><p>Under AWS MSK, I create a Kafka cluster <code>kafka-msk</code> under the VPC created in the above step and choose 2 for Number of Availability Zones. For these two Availability Zones in the VPC, choose the private subnets for each. Also set 1 broker per Availability Zone. The screenshot below reflects the configuration of the cluster:<br><img src="/blog/indepth-study/msk_config.png" alt="msk"></p>
<h3 id="2-3-3-Create-an-EC2-instance-and-connect-to-the-Kafka-cluster"><a href="#2-3-3-Create-an-EC2-instance-and-connect-to-the-Kafka-cluster" class="headerlink" title="2.3.3. Create an EC2 instance and connect to the Kafka cluster"></a>2.3.3. Create an EC2 instance and connect to the Kafka cluster</h3><p>To access the Kafka cluster, I create an EC2 instance as a client machine under the same VPC as the cluster. From local Terminal, connect to the EC2 instance and set up the Kafka environment with the following code:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Install Java </span>
sudo yum install java<span class="token number">-1.8</span><span class="token punctuation">.</span><span class="token number">0</span>

<span class="token comment" spellcheck="true"># Download Apache Kafka</span>
wget https<span class="token punctuation">:</span><span class="token operator">//</span>archive<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>org<span class="token operator">/</span>dist<span class="token operator">/</span>kafka<span class="token operator">/</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">/</span>kafka_2<span class="token number">.13</span><span class="token operator">-</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0.</span>tgz

<span class="token comment" spellcheck="true"># Run the following command in the directory where downloaded the TAR file in the previous step</span>
tar <span class="token operator">-</span>xzf kafka_2<span class="token number">.13</span><span class="token operator">-</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0.</span>tgz

<span class="token comment" spellcheck="true"># Rename the kafka_2.13-2.6.0 folder as kafka</span>
mv kafka_2<span class="token number">.13</span><span class="token operator">-</span><span class="token number">2.6</span><span class="token punctuation">.</span><span class="token number">0</span> kafka

<span class="token comment" spellcheck="true"># Go to the kafka folder</span>
cd kafka<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>kafka</code> folder contains the environment library to run the Kafka functions. To connect to the cluster <code>kafka-msk</code> via Bootstrap server or Zookeeper connection, first need to retrieve the client information by clicking the <code>View client information</code> on the <code>kafka-msk</code> cluster Details page, which will give the following information:<br><img src="/blog/indepth-study/msk_client_info.png" alt="msk_client"></p>
<p>Copy the TLS host/port pairs information under the Bootstrap servers:</p>
<pre><code>b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094</code></pre>
<p>Copy the Plaintext host/port pairs information under the ZooKeeper connection:</p>
<pre><code>z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181</code></pre>
<p>In the EC2 instance under <code>kafka</code> directory, run the following code to create a Kafka topic (also named as <code>quickstart</code>)</p>
<pre><code>bin/kafka-topics.sh --create --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --replication-factor 1 --partitions 1 --topic quickstart</code></pre>
<p>Then, run the describe command to make sure the topic is created successfully.</p>
<pre><code>bin/kafka-topics.sh --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --describe --topic quickstart</code></pre>
<p>The following screenshot reflects the existence of topic <code>quickstart</code>:<br><img src="/blog/indepth-study/msk_topic.png" alt="topic-quickstart"></p>
<p>To delete the topic just created, following code do that job:</p>
<pre><code>bin/kafka-topics.sh --delete --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --topic quickstart</code></pre>
<p>To talk to the MSK cluster, we need to use the JVM truststore. To do this, first create a folder named <code>/tmp</code> on the client machine. Then, go to the <code>bin</code> folder of the Apache Kafka installation and run the following command:</p>
<pre><code>cp /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.amzn2.0.1.x86_64/jre/lib/security/cacerts /tmp/kafka.client.truststore.jks</code></pre>
<p>While still in the <code>bin</code> folder of the Apache Kafka installation on the client machine, create a text file named <code>client.properties</code> with the following contents:</p>
<pre><code>security.protocol=SSL
ssl.truststore.location=/tmp/kafka.client.truststore.jks</code></pre>
<h3 id="2-3-4-Produce-and-consume-messages-from-the-topic"><a href="#2-3-4-Produce-and-consume-messages-from-the-topic" class="headerlink" title="2.3.4. Produce and consume messages from the topic"></a>2.3.4. Produce and consume messages from the topic</h3><h4 id="2-3-4-1-Produce-messages-to-the-created-topic"><a href="#2-3-4-1-Produce-messages-to-the-created-topic" class="headerlink" title="2.3.4.1. Produce messages to the created topic"></a>2.3.4.1. Produce messages to the created topic</h4><p><strong>Go to the <code>bin</code> folder first.</strong> Similarly, initialize the Kafka producer console, but listen to the cluster <code>kafka-msk</code> in MSK this time using Bootstrap server at topic <code>qucikstart</code>. And then input the same three messages:</p>
<pre><code>./kafka-console-producer.sh --bootstrap-server b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094 --producer.config client.properties --topic quickstart
&gt;This is the first event.
&gt;This is the second event.
&gt;This is the third event.</code></pre>
<p>The screenshot below reflects the process of producing messages:<br><img src="/blog/indepth-study/msk_producer.png" alt="msk_producer"></p>
<h4 id="2-3-4-2-Consume-the-messages-produced-by-the-producer"><a href="#2-3-4-2-Consume-the-messages-produced-by-the-producer" class="headerlink" title="2.3.4.2. Consume the messages produced by the producer"></a>2.3.4.2. Consume the messages produced by the producer</h4><p>Still stay in the <code>bin</code> folder. Similarly, initialize the Kafka consumer console, and listen to the cluster <code>kafka-msk</code> at topic <code>qucikstart</code> from beginning:</p>
<pre><code>./kafka-console-consumer.sh --bootstrap-server b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094 --consumer.config client.properties --from-beginning --topic quickstart</code></pre>
<p>The screenshot below reflects that the consumer successfully received the messages:<br><img src="/blog/indepth-study/msk_consumer.png" alt="msk_comsumer"></p>
<h3 id="2-3-5-Using-Amazon-MSK-as-an-event-source-for-AWS-Lambda"><a href="#2-3-5-Using-Amazon-MSK-as-an-event-source-for-AWS-Lambda" class="headerlink" title="2.3.5. Using Amazon MSK as an event source for AWS Lambda"></a>2.3.5. Using Amazon MSK as an event source for AWS Lambda</h3><p>Follow <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/compute/using-amazon-msk-as-an-event-source-for-aws-lambda/">this blog post</a> for most parts of configuration to allow a Lambda function to be triggered by new messages in a MSK cluster.</p>
<h4 id="2-3-5-1-Required-Lambda-function-permissions"><a href="#2-3-5-1-Required-Lambda-function-permissions" class="headerlink" title="2.3.5.1. Required Lambda function permissions"></a>2.3.5.1. Required Lambda function permissions</h4><p>The Lambda must have permission to describe VPCs and security groups, and manage elastic network interfaces. To access the Amazon MSK data stream, the Lambda function also needs two Kafka permissions: kafka:DescribeCluster and kafka:GetBootstrapBrokers. The policy template <code>AWSLambdaMSKExecutionRole</code> includes these permissions.</p>
<p>Therefore, I create a role in IAM as <code>msk_lambda</code> and attach the policy template <code>AWSLambdaMSKExecutionRole</code> with it for later used as the Execution role for the Lambda function.</p>
<h4 id="2-3-5-2-Configure-the-Lambda-event-source-mapping"><a href="#2-3-5-2-Configure-the-Lambda-event-source-mapping" class="headerlink" title="2.3.5.2. Configure the Lambda event source mapping"></a>2.3.5.2. Configure the Lambda event source mapping</h4><p>First, I create a Lambda function <code>Kafka_Lambda</code> with runtime as Python 3.7. Meanwhile, make sure to attach the function under the same VPC and two private subnets as showed in the screenshot below:<br><img src="/blog/indepth-study/lambda_vpc.png" alt="lambda_vpc"></p>
<p>Next, in the Terminal window that connected to the <code>kafka-msk</code> cluster, create a new topic as <code>kafka-lambda</code> using:</p>
<pre><code>bin/kafka-topics.sh --create --zookeeper z-3.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181,z-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:2181 --replication-factor 1 --partitions 1 --topic kafka-msk</code></pre>
<p>Under the Lambda function <code>Kafka_Lambda</code> Configuration tab, add MSK cluster <code>kafka-msk</code>‘s topic <code>kafka-lambda</code> as the event source as showed below:<br><img src="/blog/indepth-study/lambda_trigger.png" alt="trigger"></p>
<h4 id="2-3-5-3-Produce-a-message-in-the-topic-to-trigger-the-Lambda-function"><a href="#2-3-5-3-Produce-a-message-in-the-topic-to-trigger-the-Lambda-function" class="headerlink" title="2.3.5.3. Produce a message in the topic to trigger the Lambda function"></a>2.3.5.3. Produce a message in the topic to trigger the Lambda function</h4><p>Connect to the EC2 instance and under the <code>bin</code> folder of the kafka environment, initialize the Kafka producer console and rpoduce a message in the topic <code>kafka-lambda</code>:</p>
<pre><code>./kafka-console-producer.sh --bootstrap-server b-1.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094,b-2.kafka-msk.hfwvke.c11.kafka.us-east-1.amazonaws.com:9094 --producer.config client.properties --topic kafka-lambda
&gt;This is a test message to trigger Lambda function.</code></pre>
<p>Then, back to the Lambda function and choose to view logs in CloudWatch. Select the latest log stream gives:<br><img src="/blog/indepth-study/log.png" alt="log"></p>
<p>From the screenshot above, it is clear that the Lambda function was successfully trigger by the MSK cluster. The Lambda function’s event payload contains an array of records. Each array item contains details of the topic and Kafka partition identifier, together with a timestamp and base64 encoded message. Following code deployed under the Lambda function print out the <code>event</code> and <code>records</code> content, also decode the base64 encoded message that under the <code>value</code> attribute.</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> json
<span class="token keyword">import</span> base64

<span class="token keyword">def</span> <span class="token function">lambda_handler</span><span class="token punctuation">(</span>event<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span>
    record <span class="token operator">=</span> event<span class="token punctuation">[</span><span class="token string">'records'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'kafka-lambda-0'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>record<span class="token punctuation">)</span>
    msg <span class="token operator">=</span> record<span class="token punctuation">[</span><span class="token string">'value'</span><span class="token punctuation">]</span>
    msg_bytes <span class="token operator">=</span> base64<span class="token punctuation">.</span>b64decode<span class="token punctuation">(</span>msg<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'ascii'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    message <span class="token operator">=</span> msg_bytes<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'ascii'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>message<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</p></div><div class="pagination"><p class="clearfix"><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/blog/matrix-factorization/" title="Decode the Secrets Behind Recommender Systems">Next post: Decode the Secrets Behind Recommender Systems&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2020&nbsp;<a target="_blank" href="https://j-an.org/" rel="noopener noreferrer">J. An</a></p></footer></div></div></div></div><script src="/blog/js/jquery-3.1.0.min.js"></script><script src="/blog/js/bootstrap.min.js"></script><script src="/blog/js/jquery-migrate-1.2.1.min.js"></script><script src="/blog/js/jquery.appear.js"></script><script src="/blog/js/google-analytics.js"></script><script src="/blog/js/typography.js"></script></body></html>